{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container {width:98% !important;}</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container {width:98% !important;}</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 자연어처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사람하고 비슷하게 말하게끔 챗봇 만들려면? ~데이터 정제  \n",
    "\n",
    "상담용 챗봇 만들려면? 데이터 정제(특히, 정규화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lemmatization : 동사 원형으로 만들기  \n",
    "\n",
    "- 반복되는 글자 통합  \n",
    "\n",
    "- 숫자 num으로 통합  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    ": 분리  \n",
    "문장 토큰화  \n",
    "단어 토큰화    \n",
    "음절 토큰화(subword tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text=\"I never thought through love we'd be. Making one as lovely as she. But isn't she lovely made from love.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I never thought through love we'd be\",\n",
       " 'Making one as lovely as she',\n",
       " \"But isn't she lovely made from love.\"]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = sample_text.split('. ')\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어 토큰화 (split()하면 자동으로 띄어쓰기 무시하고 잘라줌)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'never',\n",
       " 'thought',\n",
       " 'through',\n",
       " 'love',\n",
       " \"we'd\",\n",
       " 'be.',\n",
       " 'Making',\n",
       " 'one',\n",
       " 'as',\n",
       " 'lovely',\n",
       " 'as',\n",
       " 'she.',\n",
       " 'But',\n",
       " \"isn't\",\n",
       " 'she',\n",
       " 'lovely',\n",
       " 'made',\n",
       " 'from',\n",
       " 'love.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence = sample_text.split()\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영문 Tokenization  \n",
    "\n",
    "```\n",
    "노트북 설치\n",
    "!pip install nltk\n",
    "```\n",
    "\n",
    "JVM(자바 가상 환경)이 설치 되어 있어야 함!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\CPB06GameN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Ain't nothin' sweeter, you want this sugar, don't ya?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Ain't\", \"nothin'\", 'sweeter,', 'you', 'want', 'this', 'sugar,', \"don't\", 'ya?']\n"
     ]
    }
   ],
   "source": [
    "print(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ain', \"'\", 't', 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'don', \"'\", 't', 'ya', '?']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tokenizer = WordPunctTokenizer()\n",
    "print(tokenizer.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree bank word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ai', \"n't\", 'nothin', \"'\", 'sweeter', ',', 'you', 'want', 'this', 'sugar', ',', 'do', \"n't\", 'ya', '?']\n",
      "['I', \"'m\", 'Iron-man']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "print(tokenizer.tokenize(sentence))\n",
    "print(tokenizer.tokenize(\"I'm Iron-man\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Korean Tokenization\n",
    "\n",
    "교착어  \n",
    "띄어쓰기  \n",
    "어순  \n",
    "다의어  \n",
    "\n",
    "```\n",
    "!pip install konlpy\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: KoNLPy in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from KoNLPy) (0.4.4)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from KoNLPy) (1.0.2)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from KoNLPy) (4.6.1)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from KoNLPy) (4.6.0)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from KoNLPy) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from KoNLPy) (1.19.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->KoNLPy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->KoNLPy) (2.24.0)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->KoNLPy) (1.15.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->KoNLPy) (3.1.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (2020.6.20)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->KoNLPy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install KoNLPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### konlpy tokenizer 종류(모두 덕타이핑 지원)\n",
    "- Okt(트위터)\n",
    "- Kkma(꼬꼬마)\n",
    "- Komoran(코모란)\n",
    "- Hannanum(한나눔)\n",
    "- mecap(메켑)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum, Kkma, Komoran, Okt\n",
    "\n",
    "hannanum = Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()\n",
    "okt = Okt()\n",
    "\n",
    "sentence = '좋으니 그 사람 솔직히 견디기 버거워'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tokenizer(tokenizer, s):\n",
    "    print(tokenizer.nouns(s))\n",
    "    print(tokenizer.morphs(s)) # 형태소 = 명사 동사 등 모두 포함\n",
    "    print(tokenizer.pos(s)) # 종류, 형태소 튜플로 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['그', '사람']\n",
      "['좋으니', '그', '사람', '솔직히', '견디기', '버거워']\n",
      "[('좋으니', 'Adjective'), ('그', 'Noun'), ('사람', 'Noun'), ('솔직히', 'Adjective'), ('견디기', 'Verb'), ('버거워', 'Adjective')]\n",
      "['사람']\n",
      "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버겁', '어']\n",
      "[('좋', 'VA'), ('으니', 'ECD'), ('그', 'MDT'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버겁', 'VA'), ('어', 'ECS')]\n",
      "['사람']\n",
      "['좋', '으니', '그', '사람', '솔직히', '견디', '기', '버거워']\n",
      "[('좋', 'VA'), ('으니', 'EC'), ('그', 'MM'), ('사람', 'NNG'), ('솔직히', 'MAG'), ('견디', 'VV'), ('기', 'ETN'), ('버거워', 'NA')]\n"
     ]
    }
   ],
   "source": [
    "print_tokenizer(okt, sentence)\n",
    "print_tokenizer(kkma, sentence)\n",
    "print_tokenizer(komoran, sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 문장 tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kss in c:\\programdata\\anaconda3\\lib\\site-packages (2.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss\n",
    "text = '제 아이피는 192.168.56.51 이예요. 자연어 처리가 재미있나요?ㅋㅋㅋ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['제 아이피는 192.168.56.51 이예요.', '자연어 처리가 재미있나요?ㅋㅋㅋ']\n"
     ]
    }
   ],
   "source": [
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 맞춤법 정리\n",
    "\n",
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "\n",
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/ssut/py-hanspell.git\n",
      "  Cloning https://github.com/ssut/py-hanspell.git to c:\\users\\cpb06gamen\\appdata\\local\\temp\\pip-req-build-7v434yap\n",
      "Requirement already satisfied (use --upgrade to upgrade): py-hanspell==1.1 from git+https://github.com/ssut/py-hanspell.git in c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from py-hanspell==1.1) (2.24.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (1.25.11)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->py-hanspell==1.1) (3.0.4)\n",
      "Building wheels for collected packages: py-hanspell\n",
      "  Building wheel for py-hanspell (setup.py): started\n",
      "  Building wheel for py-hanspell (setup.py): finished with status 'done'\n",
      "  Created wheel for py-hanspell: filename=py_hanspell-1.1-py3-none-any.whl size=4899 sha256=17dfa3e6c8ed1a754ca1dfc3e19cdaa4edeb240615c6222d0a21d4c2b662f383\n",
      "  Stored in directory: C:\\Users\\CPB06GameN\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-vuf_ee17\\wheels\\3f\\a5\\73\\e4d2806ae141d274fdddaabf8c0ed79be9357d36bfdc99e4b4\n",
      "Successfully built py-hanspell\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/haven-jeon/PyKoSpacing.git\n",
      "  Cloning https://github.com/haven-jeon/PyKoSpacing.git to c:\\users\\cpb06gamen\\appdata\\local\\temp\\pip-req-build-jduk4twg\n",
      "Requirement already satisfied (use --upgrade to upgrade): pykospacing==0.5 from git+https://github.com/haven-jeon/PyKoSpacing.git in c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: tensorflow==2.5.0 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from pykospacing==0.5) (2.5.0)\n",
      "Requirement already satisfied: h5py==3.1.0 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from pykospacing==0.5) (3.1.0)\n",
      "Requirement already satisfied: argparse>=1.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pykospacing==0.5) (1.4.0)\n",
      "Requirement already satisfied: tensorboard~=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.5.0)\n",
      "Requirement already satisfied: wrapt~=1.12.1 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.12.1)\n",
      "Requirement already satisfied: google-pasta~=0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.2.0)\n",
      "Requirement already satisfied: six~=1.15.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.5.0)\n",
      "Requirement already satisfied: grpcio~=1.34.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.34.1)\n",
      "Requirement already satisfied: gast==0.4.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.4.0)\n",
      "Requirement already satisfied: absl-py~=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.13.0)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.3.0)\n",
      "Requirement already satisfied: astunparse~=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing~=1.1.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.1.2)\n",
      "Requirement already satisfied: wheel~=0.35 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (0.35.1)\n",
      "Requirement already satisfied: typing-extensions~=3.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.7.4.3)\n",
      "Requirement already satisfied: flatbuffers~=1.12.0 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.12)\n",
      "Requirement already satisfied: numpy~=1.19.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.19.2)\n",
      "Requirement already satisfied: keras-nightly~=2.5.0.dev in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (2.5.0.dev2021032900)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (3.17.3)\n",
      "Requirement already satisfied: termcolor~=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow==2.5.0->pykospacing==0.5) (1.1.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.32.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.3.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.0.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.8.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.4.4)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (50.3.1.post20201107)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.6.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.2.8)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (4.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.3.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (2.10)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0->pykospacing==0.5) (3.1.1)\n",
      "Building wheels for collected packages: pykospacing\n",
      "  Building wheel for pykospacing (setup.py): started\n",
      "  Building wheel for pykospacing (setup.py): finished with status 'done'\n",
      "  Created wheel for pykospacing: filename=pykospacing-0.5-py3-none-any.whl size=2255674 sha256=4560b3ad309b9ad0bb40ccebb31e904df8262f1763d5fdbcc71cc218384e8c00\n",
      "  Stored in directory: C:\\Users\\CPB06GameN\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-a94uq5m7\\wheels\\79\\a0\\33\\16f2cd03d21f76a663f5d69a0b96f0351335385349136fbd03\n",
      "Successfully built pykospacing\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/haven-jeon/PyKoSpacing.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import Spacing\n",
    "from hanspell import spell_checker\n",
    "\n",
    "spacing = Spacing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4번 놀고 있지.4번은 팀워크가 없어.4번은 개인주의야.4번은 혼자 밖에 생각하지 않아.\n"
     ]
    }
   ],
   "source": [
    "text = '4번놀고있지.4번은팀워크가없어.4번은개인주의야.4번은혼자밖에생각하지않아.'\n",
    "spacing_text = spacing(text)\n",
    "print(spacing_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이건 제대로 나옴 '혼자밖에'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4번 놀고 있지. 4번은 팀워크가 없어. 4번은 개인주의야. 4번은 혼자밖에 생각하지 않아.\n"
     ]
    }
   ],
   "source": [
    "hanspell_text = spell_checker.check(text)\n",
    "print(hanspell_text.checked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "맞춤법 검사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "맞춤법 틀리면 왜 안돼?\n"
     ]
    }
   ],
   "source": [
    "text = '맞춤뻡 틀리면 외 않되?'\n",
    "hanspell_text = spell_checker.check(text).checked\n",
    "print(hanspell_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 영어 문장 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Since I'm actively looking for Ph.D. students. I get the same question a dozen times every year.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Since I'm actively looking for Ph.D. students.\", 'I get the same question a dozen times every year.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My IP Address is 192.168.56.51.', 'Hello World!']\n"
     ]
    }
   ],
   "source": [
    "text = 'My IP Address is 192.168.56.51. Hello World!'\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming (어간추출)\n",
    "\n",
    "English 정규화  \n",
    "\n",
    "동사 하나로 통일 등등(lemmitation)\n",
    "\n",
    "\n",
    "**문장 복잡도를 낮춘다**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming (어간 추출)\n",
    "\n",
    "- 주의! 사전에 없는 단어가 생기기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "text = \"This was not the map we found in Billy Bones's chest, but an accurate copy, complete in all things--names and heights and soundings--with the single exception of the red crosses and the written notes.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'was', 'not', 'the', 'map', 'we', 'found', 'in', 'Billy', 'Bones', \"'s\", 'chest', ',', 'but', 'an', 'accurate', 'copy', ',', 'complete', 'in', 'all', 'things', '--', 'names', 'and', 'heights', 'and', 'soundings', '--', 'with', 'the', 'single', 'exception', 'of', 'the', 'red', 'crosses', 'and', 'the', 'written', 'notes', '.']\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['thi', 'wa', 'not', 'the', 'map', 'we', 'found', 'in', 'billi', 'bone', \"'s\", 'chest', ',', 'but', 'an', 'accur', 'copi', ',', 'complet', 'in', 'all', 'thing', '--', 'name', 'and', 'height', 'and', 'sound', '--', 'with', 'the', 'singl', 'except', 'of', 'the', 'red', 'cross', 'and', 'the', 'written', 'note', '.']\n"
     ]
    }
   ],
   "source": [
    "stem_list = [porter_stemmer.stem(w) for w in words]\n",
    "print(stem_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stem은 단순 분류나 회귀문제를 풀 때 도움이 된다  \n",
    "\n",
    "자연어 생성 모델을 만들 때는 사용하면 안 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['serrial', 'allow', 'allow', 'medic', 'thi', 'pretti', 'beauti']\n"
     ]
    }
   ],
   "source": [
    "words = ['serrialize', 'allowance', 'allowed', 'medical', 'this', 'pretty', 'beautiful']\n",
    "print([porter_stemmer.stem(w) for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한국어 정규화 - Okt 사용  \n",
    "\n",
    "Stemming(어간추출), Normalization(받침제거)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이', '것', '도', '모르고', '저', '것', '도', '모르고', '아무', '것', '도', '모른다면', '뭘', '모르는지도', '모르겠더라']\n",
      "['이', '것', '도', '모르다', '저', '것', '도', '모르다', '아무', '것', '도', '모르다', '뭘', '모르다', '모르다']\n"
     ]
    }
   ],
   "source": [
    "okt = Okt()\n",
    "text = '이것도 모르고 저것도 모르고 아무것도 모른다면 뭘 모르는지도 모르겠더라'\n",
    "\n",
    "print(okt.morphs(text))\n",
    "print(okt.morphs(text, stem=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르고', 'Verb'), ('저', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르고', 'Verb'), ('아무', 'Modifier'), ('것', 'Noun'), ('도', 'Josa'), ('모른다면', 'Verb'), ('뭘', 'Noun'), ('모르는지도', 'Verb'), ('모르겠더라', 'Verb')]\n",
      "[('이', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르다', 'Verb'), ('저', 'Determiner'), ('것', 'Noun'), ('도', 'Josa'), ('모르다', 'Verb'), ('아무', 'Modifier'), ('것', 'Noun'), ('도', 'Josa'), ('모르다', 'Verb'), ('뭘', 'Noun'), ('모르다', 'Verb'), ('모르다', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(text))\n",
    "print(okt.pos(text, stem=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 정규화  \n",
    "\n",
    "받침 제거용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('딥', 'Noun'), ('러닝', 'Noun'), ('진짜', 'Noun'), ('어렵닼', 'Noun'), ('ㅋㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('여려울지', 'Adjective'), ('몰랐어', 'Verb'), ('옄', 'Noun'), ('ㅋㅋㅋㅋ', 'KoreanParticle')]\n",
      "[('딥', 'Noun'), ('러닝', 'Noun'), ('진짜', 'Noun'), ('어렵다', 'Adjective'), ('ㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('여려울지', 'Adjective'), ('몰랐어여', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n",
      "[('딥', 'Noun'), ('러닝', 'Noun'), ('진짜', 'Noun'), ('어렵닼', 'Noun'), ('ㅋㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('여리다', 'Adjective'), ('모르다', 'Verb'), ('옄', 'Noun'), ('ㅋㅋㅋㅋ', 'KoreanParticle')]\n",
      "[('딥', 'Noun'), ('러닝', 'Noun'), ('진짜', 'Noun'), ('어렵다', 'Adjective'), ('ㅋㅋㅋ', 'KoreanParticle'), ('이렇게', 'Adverb'), ('여리다', 'Adjective'), ('모르다', 'Verb'), ('ㅋㅋㅋ', 'KoreanParticle')]\n"
     ]
    }
   ],
   "source": [
    "text = '딥러닝 진짜 어렵닼ㅋㅋㅋㅋ 이렇게 여려울지 몰랐어옄ㅋㅋㅋㅋ'\n",
    "print(okt.pos(text))\n",
    "print(okt.pos(text, norm=True))\n",
    "print(okt.pos(text, stem=True))\n",
    "print(okt.pos(text, norm=True, stem=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 의미없이 반복되는 문자 정제\n",
    "\n",
    "```\n",
    "!pip install soynlp\n",
    "```\n",
    "신조어 체크 잘함 ㅇㅇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soynlp\n",
      "  Downloading soynlp-0.0.493-py3-none-any.whl (416 kB)\n",
      "Requirement already satisfied: numpy>=1.12.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from soynlp) (1.19.2)\n",
      "Requirement already satisfied: psutil>=5.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from soynlp) (5.7.2)\n",
      "Requirement already satisfied: scipy>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soynlp) (1.5.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from soynlp) (0.23.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (2.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn>=0.20.0->soynlp) (0.17.0)\n",
      "Installing collected packages: soynlp\n",
      "Successfully installed soynlp-0.0.493\n"
     ]
    }
   ],
   "source": [
    "!pip install soynlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "야ㅋㅋ 딥러닝 졸잼쓰\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import emoticon_normalize\n",
    "\n",
    "print(emoticon_normalize('얔ㅋㅋㅋㅋㅋ 딥러닝 졸잼쓰', num_repeats=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문을 쿵쿠우쿵쿠우웅쿵 두드렸다\n"
     ]
    }
   ],
   "source": [
    "from soynlp.normalizer import repeat_normalize\n",
    "\n",
    "print(repeat_normalize('문을 쿵쿠우쿵쿠우웅쿵 두드렸다', num_repeats=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정규식 텍스트 정제 Cleaning\n",
    "\n",
    "정규식을 이용해서 정제하거나, 불용어 정제한다  \n",
    "\n",
    "- 불용어 Stopwords  \n",
    "빈도수가 낮거나 짧거나 의미 없는 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "eng_sent = \"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 영어가 아닌 것들은 모두 공백으로 치환  \n",
    "sub (substitution)\n",
    "\n",
    "[] : 한 글자, 한 글자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Yeah  do you expect people to read the FAQ  etc  and actually accept hard atheism   No  you need a little leap of faith  Jimmy   Your logic runs out of steam         Jim   Sorry I can t pity you  Jim   And I m sorry that you have these feelings of denial about the faith you need to get by   Oh well  just pretend that it will all end happily ever after anyway   Maybe if you start a new newsgroup  alt atheist hard  you won t be bummin  so much        Bye Bye  Big Jim   Don t forget your Flintstone s Chewables          Bake Timmons  III\n"
     ]
    }
   ],
   "source": [
    "eng_sent = re.sub('[^a-zA-Z]', ' ', eng_sent)\n",
    "print(eng_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yeah expect people read actually accept hard atheism need little leap faith Jimmy Your logic runs steam Sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway Maybe start newsgroup atheist hard bummin much forget your Flintstone Chewables Bake Timmons\n"
     ]
    }
   ],
   "source": [
    "# 4 글자 이상인 단어만 추출하기\n",
    "eng_sent = ' '.join([w for w in eng_sent.split() if len(w) > 3])\n",
    "print(eng_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 정규식 표현  \n",
    "\n",
    "[ㄱ-ㅎㅏ-ㅣ가-힣]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이것도 쉽지 않네  이거 실화냐  ㅋㅋ ㄴ그ㅡ 서장 남청동 살제  내가 임마 느그 스장이랑 으이이이              '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sent = '와 이것도 쉽지 않네. 이거 실화냐? ㅋㅋ ㄴ그ㅡ 서장 남청동 살제? 내가 임마 느그 스장이랑 으이이이?! hello world'\n",
    "kor_sent = re.sub('[^ㄱ-ㅎㅏ-ㅣ가-힣]', ' ', kor_sent)\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 공백이 2개 이상 -> 1개 치환  \n",
    "\n",
    "중괄호 : 반복 정규식 {From 2 to inf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이것도 쉽지 않네 이거 실화냐 ㅋㅋ ㄴ그ㅡ 서장 남청동 살제 내가 임마 느그 스장이랑 으이이이 '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sent = re.sub('[ ]{2,}', ' ', kor_sent)\n",
    "kor_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'와 이것도 쉽지 않네 이거 실화냐 ㅋㅋ ㄴ그ㅡ 서장 남청동 살제 내가 임마 느그 스장이랑 으이이이'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kor_sent.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopword 정제\n",
    "주어는 분석에 주로 필요없다.  \n",
    "무엇을 했는지에 관심이 있기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\CPB06GameN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # 불용어 사전 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = \"Family is not an important thing. It's everything\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['family', 'important', 'thing', '.', \"'s\", 'everything']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_token = word_tokenize(example.lower()) #문장을 소문자로 만든 다음 토큰화\n",
    "\n",
    "# 불용어 사전에 들어있지 않은 단어면 추가\n",
    "result = [w for w in word_token if w not in stop_words]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 stopword 정제  \n",
    "\n",
    "개발자가 직접 정의하는게 일반적\n",
    "- stopword는 내가 직접 vs normalization 패키지가 알아서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['내', '패', '와', '정마담', '패', '를', '밑', '에서', '뺏지', '.', '내', '가', '빙', '다리', '핫', '바리', '로', '보이냐', '?']\n"
     ]
    }
   ],
   "source": [
    "example = '내패와 정마담 패를 밑에서 뺏지. 내가 빙다리 핫바리로 보이냐?'\n",
    "example_spell_check = spell_checker.check(example).checked\n",
    "\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원문 :  ['내', '패', '와', '정마담', '패', '를', '밑', '에서', '뺏지', '.', '내', '가', '빙', '다리', '핫', '바리', '로', '보이냐', '?']\n",
      "제거 후 :  ['내', '패', '정마담', '패', '밑', '에서', '뺏지', '.', '내', '빙', '다리', '핫', '바리', '로', '보이냐', '?']\n"
     ]
    }
   ],
   "source": [
    "stop_words = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다', '것', '게']\n",
    "result = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "print('원문 : ', word_tokens)\n",
    "print('제거 후 : ', result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 정수 인코딩(integer encoding)\n",
    "\n",
    "고유 정수 부여?\n",
    "- 빈도수(대부분)\n",
    "- 가나다 순\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### padding\n",
    "단어, 문장 길이를 맞춰줘야 같은 배열로 묶을 수 있음  \n",
    "그래야 gpu로 병렬 연산하지  \n",
    "\n",
    "pad = 0\n",
    "\n",
    "pre-padding(tensorflow), post-padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV (out of vocabulary)\n",
    "\n",
    "모든 단어를 단어 집합에 다 넣으면?  \n",
    "\n",
    "용량이 너무 커짐!  \n",
    "\n",
    "그러면??  \n",
    "\n",
    "OOV 토큰을 만들어서 mapping 해주면 된다\n",
    "\n",
    "[오늘, 회식] <- 기존 단어 뭉치  \n",
    "[오늘, 회식, **소고기, 양고기, 돼지고기**] <- 3개 추가\n",
    "\n",
    "소고기, 양고기, 돼지고기는 **OOV 하나 매핑해서** 퉁친다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 부록 (정규표현식)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = 'You can do it!'\n",
    "m = re.match('You', source)\n",
    "\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'너무 내맘모르고 너무해 너무해'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source = '이런 너무 내맘모르고 너무해 너무해'\n",
    "m = re.search('너.*', source)\n",
    "\n",
    "m.group()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bee', 'bee', 'be ', 'bee', 'bee', 'be ']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyrics = \"Lately, I've been, I've been losing sleep \\n Dreaming about the things  \\n that we could be \\n But baby, I've been, I've been praying hard  \\n Said no more counting dollars \\n We'll be counting stars\"\n",
    "m = re.findall('be.?', lyrics)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lately, I've \",\n",
       " \"n, I've \",\n",
       " 'n losing sleep \\n Dreaming about the things  \\n that we could ',\n",
       " \"\\n But baby, I've \",\n",
       " \"n, I've \",\n",
       " \"n praying hard  \\n Said no more counting dollars \\n We'll \",\n",
       " 'counting stars']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.split('be.?', lyrics)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Lately, I've firen, I've firen losing sleep \\n Dreaming about the things  \\n that we could fir \\n But baby, I've firen, I've firen praying hard  \\n Said no more counting dollars \\n We'll fir counting stars\""
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.sub('be.{0}', 'fir', lyrics)\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Lately, I've been, I've been losing sleep \",\n",
       " \" But baby, I've been, I've been praying hard  \"]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = re.findall('[^a-c]', lyrics)\n",
    "m = re.findall('be*', lyrics)\n",
    "m = re.findall('be{1,4}', lyrics)\n",
    "m = re.findall('l\\w+|b\\w+', lyrics)\n",
    "m = re.findall('^.+I.+', lyrics, re.MULTILINE)\n",
    "m"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "166.491px",
    "left": "1414.5px",
    "right": "20px",
    "top": "-0.0113637px",
    "width": "338.977px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
