{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integer Encoding\n",
    "\n",
    "- Integer Encoding\n",
    "    * 단어 토큰에 고유한 정수 부여\n",
    "    * ~중복 허용\n",
    "    * OOV와 결합가능\n",
    "    * padding : 문장마다 길이가 다른 경우 0을 이용해 보정\n",
    "\n",
    "- 무슨 기준으로 Integer Encoding할 때 정수를 부여할까?\n",
    "    * 가나다 순\n",
    "    * 빈도 순"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##### text 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"\"\"Isn't she lovely.\n",
    "Isn't she wonderful.\n",
    "Isn't she precious.\n",
    "Less than one minute old.\n",
    "I never thought through love we'd be.\n",
    "Making one as lovely as she.\n",
    "But isn't she lovely made from love.\n",
    "Isn't she pretty.\n",
    "Truly the angel's best.\n",
    "Boy, I'm so happy.\n",
    "We have been heaven blessed.\n",
    "I can't believe what God has done.\n",
    "Through us he's given life to one.\n",
    "But isn't she lovely made from love.\n",
    "Isn't she lovely.\n",
    "Life and love are the same.\n",
    "Life is Aisha.\n",
    "The meaning of her name.\n",
    "Londie, it could have not been done.\n",
    "Without you who conceived the one.\n",
    "That's so very lovely made from love.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Isn't she lovely.\", \"Isn't she wonderful.\", \"Isn't she precious.\", 'Less than one minute old.', \"I never thought through love we'd be.\", 'Making one as lovely as she.', \"But isn't she lovely made from love.\", \"Isn't she pretty.\", \"Truly the angel's best.\", \"Boy, I'm so happy.\", 'We have been heaven blessed.', \"I can't believe what God has done.\", \"Through us he's given life to one.\", \"But isn't she lovely made from love.\", \"Isn't she lovely.\", 'Life and love are the same.', 'Life is Aisha.', 'The meaning of her name.', 'Londie, it could have not been done.', 'Without you who conceived the one.', \"That's so very lovely made from love.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sent_tokens = sent_tokenize(text)\n",
    "print(sent_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word = set(stopwords.words('english')) #불용어 집합화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어를 토큰화 시켜서 리스트 형태로 저장하겠다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[\"n't\", 'lovely'], [\"n't\", 'wonderful'], [\"n't\", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love'], ['making', 'one', 'lovely'], [\"n't\", 'lovely', 'made', 'love'], [\"n't\", 'pretty'], ['truly', 'angel', 'best'], ['boy', 'happy'], ['heaven', 'blessed'], [\"n't\", 'believe', 'god', 'done'], ['given', 'life', 'one'], [\"n't\", 'lovely', 'made', 'love'], [\"n't\", 'lovely'], ['life', 'love'], ['life', 'aisha'], ['meaning', 'name'], ['londie', 'could', 'done'], ['without', 'conceived', 'one'], ['lovely', 'made', 'love']]\n",
      "['lovely', 'made', 'love']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for sent in sent_tokens:\n",
    "    word_tokens = word_tokenize(sent) # 각 문장을 단어 토큰화\n",
    "    result = [] # 불용어 제거, 길이가 짧은 단어 제거한 데이터를 담을 리스트\n",
    "    \n",
    "    for word in word_tokens:\n",
    "        word = word.lower() # 소문자 = 정규화\n",
    "\n",
    "        if word not in stop_word: # 불용어 처리\n",
    "            if len(word) > 2 : # 단어 길이 2이상만\n",
    "                result.append(word)\n",
    "        \n",
    "    sentences.append(result)\n",
    "print(sentences)\n",
    "print(result) # for문 안에서 result는 리셋되기 때문에 맨 마지막 것만 살아남음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 단어 집합\n",
    "\n",
    "규칙 : 빈도가 높을 수록 앞에 오도록 만들 예정\n",
    "\n",
    "sum 응용 -> counter -> sorted 응용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"n't\", 'lovely', \"n't\", 'wonderful', \"n't\", 'precious', 'less', 'one', 'minute', 'old', 'never', 'thought', 'love', 'making', 'one', 'lovely', \"n't\", 'lovely', 'made', 'love', \"n't\", 'pretty', 'truly', 'angel', 'best', 'boy', 'happy', 'heaven', 'blessed', \"n't\", 'believe', 'god', 'done', 'given', 'life', 'one', \"n't\", 'lovely', 'made', 'love', \"n't\", 'lovely', 'life', 'love', 'life', 'aisha', 'meaning', 'name', 'londie', 'could', 'done', 'without', 'conceived', 'one', 'lovely', 'made', 'love']\n"
     ]
    }
   ],
   "source": [
    "words = sum(sentences, [])\n",
    "# sum을 하는 순간 속 리스트가 하나 해제된다.\n",
    "# list에 더한다? =append이기 때문에 그 뒤에 extend처럼 뒤에 붙는거다.\n",
    "# sum([1,2,3], 10) 1,2,3 더한 다음 10에 더한다\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({\"n't\": 8,\n",
       "         'lovely': 6,\n",
       "         'wonderful': 1,\n",
       "         'precious': 1,\n",
       "         'less': 1,\n",
       "         'one': 4,\n",
       "         'minute': 1,\n",
       "         'old': 1,\n",
       "         'never': 1,\n",
       "         'thought': 1,\n",
       "         'love': 5,\n",
       "         'making': 1,\n",
       "         'made': 3,\n",
       "         'pretty': 1,\n",
       "         'truly': 1,\n",
       "         'angel': 1,\n",
       "         'best': 1,\n",
       "         'boy': 1,\n",
       "         'happy': 1,\n",
       "         'heaven': 1,\n",
       "         'blessed': 1,\n",
       "         'believe': 1,\n",
       "         'god': 1,\n",
       "         'done': 2,\n",
       "         'given': 1,\n",
       "         'life': 3,\n",
       "         'aisha': 1,\n",
       "         'meaning': 1,\n",
       "         'name': 1,\n",
       "         'londie': 1,\n",
       "         'could': 1,\n",
       "         'without': 1,\n",
       "         'conceived': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter # 횟수 세기 용도\n",
    "vocab = Counter(words)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"n't\", 8),\n",
       " ('lovely', 6),\n",
       " ('love', 5),\n",
       " ('one', 4),\n",
       " ('made', 3),\n",
       " ('life', 3),\n",
       " ('done', 2),\n",
       " ('wonderful', 1),\n",
       " ('precious', 1),\n",
       " ('less', 1),\n",
       " ('minute', 1),\n",
       " ('old', 1),\n",
       " ('never', 1),\n",
       " ('thought', 1),\n",
       " ('making', 1),\n",
       " ('pretty', 1),\n",
       " ('truly', 1),\n",
       " ('angel', 1),\n",
       " ('best', 1),\n",
       " ('boy', 1),\n",
       " ('happy', 1),\n",
       " ('heaven', 1),\n",
       " ('blessed', 1),\n",
       " ('believe', 1),\n",
       " ('god', 1),\n",
       " ('given', 1),\n",
       " ('aisha', 1),\n",
       " ('meaning', 1),\n",
       " ('name', 1),\n",
       " ('londie', 1),\n",
       " ('could', 1),\n",
       " ('without', 1),\n",
       " ('conceived', 1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_sorted = sorted(vocab.items(), key=lambda x : x[1], reverse=True) #여기서 x는? tuple 1개\n",
    "vocab_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7}\n"
     ]
    }
   ],
   "source": [
    "word2idx = {}\n",
    "i = 0 # 실제 부여되는 인덱스\n",
    "\n",
    "for (word, freq) in vocab_sorted:\n",
    "    \n",
    "    # 빈도수 낮은 단어는 제거\n",
    "    if freq > 1:\n",
    "        i = i + 1\n",
    "        word2idx[word] = i\n",
    "        \n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 핵심\n",
    "빈도수가 가장 높은 상위 n개만 뽑아야한다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 중요 테크닉\n",
    "- vocab_size = 5\n",
    "단어집합에서 사용할 단어의 개수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['life', 'done']\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "del_word = [w for w, idx in word2idx.items() if idx > vocab_size]\n",
    "print(del_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5}\n"
     ]
    }
   ],
   "source": [
    "for w in del_word:\n",
    "    del word2idx[w]\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding 수동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, '<oov>': 6}\n"
     ]
    }
   ],
   "source": [
    "word2idx['<oov>'] = 6\n",
    "print(word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "변화 전 [[\"n't\", 'lovely'], [\"n't\", 'wonderful'], [\"n't\", 'precious'], ['less', 'one', 'minute', 'old'], ['never', 'thought', 'love']] \n",
      "변화 후 [[1, 2], [1, 6], [1, 6], [6, 4, 6, 6], [6, 6, 3]]\n"
     ]
    }
   ],
   "source": [
    "encoded = []\n",
    "\n",
    "for sent in sentences:\n",
    "    temp = []\n",
    "    \n",
    "    for w in sent:\n",
    "        if w in word2idx: # 단어집합에 단어가 있다면!\n",
    "            temp.append(word2idx[w]) # 단어에 맞는 정수를 추가해줭\n",
    "        else: #단어가 없다면?\n",
    "            temp.append(word2idx['<oov>'])\n",
    "    encoded.append(temp)\n",
    "print('변화 전 {} \\n변화 후 {}'.format(sentences[:5], encoded[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoding 자동(tf)\n",
    "\n",
    "- fit_on_texts()에 코퍼스(sentences)를 넣으면 빈도 기준으로 만들어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(sentences) # sentences는 항상 2차원이어야 한다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"n't\": 1, 'lovely': 2, 'love': 3, 'one': 4, 'made': 5, 'life': 6, 'done': 7, 'wonderful': 8, 'precious': 9, 'less': 10, 'minute': 11, 'old': 12, 'never': 13, 'thought': 14, 'making': 15, 'pretty': 16, 'truly': 17, 'angel': 18, 'best': 19, 'boy': 20, 'happy': 21, 'heaven': 22, 'blessed': 23, 'believe': 24, 'god': 25, 'given': 26, 'aisha': 27, 'meaning': 28, 'name': 29, 'londie': 30, 'could': 31, 'without': 32, 'conceived': 33} \n",
      " OrderedDict([(\"n't\", 8), ('lovely', 6), ('wonderful', 1), ('precious', 1), ('less', 1), ('one', 4), ('minute', 1), ('old', 1), ('never', 1), ('thought', 1), ('love', 5), ('making', 1), ('made', 3), ('pretty', 1), ('truly', 1), ('angel', 1), ('best', 1), ('boy', 1), ('happy', 1), ('heaven', 1), ('blessed', 1), ('believe', 1), ('god', 1), ('done', 2), ('given', 1), ('life', 3), ('aisha', 1), ('meaning', 1), ('name', 1), ('londie', 1), ('could', 1), ('without', 1), ('conceived', 1)])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index) # 인덱스\n",
    "print(tokenizer.word_counts) # 빈도수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 인코딩, 디코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [1, 8],\n",
       " [1, 9],\n",
       " [10, 4, 11, 12],\n",
       " [13, 14, 3],\n",
       " [15, 4, 2],\n",
       " [1, 2, 5, 3],\n",
       " [1, 16],\n",
       " [17, 18, 19],\n",
       " [20, 21],\n",
       " [22, 23],\n",
       " [1, 24, 25, 7],\n",
       " [26, 6, 4],\n",
       " [1, 2, 5, 3],\n",
       " [1, 2],\n",
       " [6, 3],\n",
       " [6, 27],\n",
       " [28, 29],\n",
       " [30, 31, 7],\n",
       " [32, 33, 4],\n",
       " [2, 5, 3]]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(sentences) # 인코딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"n't lovely\", \"n't precious\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[1, 2], [1, 9]]) # 디코딩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OOV 및 Padding\n",
    "\n",
    "추가했을 시,\n",
    "oov : 1번 토큰 (default)  \n",
    "pad : 0번 토큰 (default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3], [2, 1], [2, 1], [1, 5, 1, 1], [1, 1, 4], [1, 5, 3], [2, 3, 6, 4], [2, 1], [1, 1, 1], [1, 1], [1, 1], [2, 1, 1, 1], [1, 1, 5], [2, 3, 6, 4], [2, 3], [1, 4], [1, 1], [1, 1], [1, 1, 1], [1, 1, 5], [3, 6, 4]]\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 5\n",
    "\n",
    "# num_words : 단어집합 내에서 사용할 단어 개수\n",
    "tokenizer = Tokenizer(num_words=vocab_size+2, oov_token='<oov>') # +2 왜? oov와 pad 토큰 추가해야 하니까\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "print(tokenizer.texts_to_sequences(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print('{}'.format(tokenizer.word_index['<oov>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: konlpy in c:\\programdata\\anaconda3\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: beautifulsoup4==4.6.0 in c:\\users\\cpb06gamen\\appdata\\roaming\\python\\python38\\site-packages (from konlpy) (4.6.0)\n",
      "Requirement already satisfied: lxml>=4.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (4.6.1)\n",
      "Requirement already satisfied: numpy>=1.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.19.2)\n",
      "Requirement already satisfied: JPype1>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (1.0.2)\n",
      "Requirement already satisfied: tweepy>=3.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (3.10.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from konlpy) (0.4.4)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
      "Requirement already satisfied: requests[socks]>=2.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tweepy>=3.7.0->konlpy) (2.24.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.25.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in c:\\programdata\\anaconda3\\lib\\site-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ratings_test.txt', <http.client.HTTPMessage at 0x2345173fa60>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\", filename=\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6270596</td>\n",
       "      <td>굳 ㅋ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9274899</td>\n",
       "      <td>GDNTOPCLASSINTHECLUB</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8544678</td>\n",
       "      <td>뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6825595</td>\n",
       "      <td>지루하지는 않은데 완전 막장임... 돈주고 보기에는....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6723715</td>\n",
       "      <td>3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                           document  label\n",
       "0  6270596                                                굳 ㅋ      1\n",
       "1  9274899                               GDNTOPCLASSINTHECLUB      0\n",
       "2  8544678             뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아      0\n",
       "3  6825595                   지루하지는 않은데 완전 막장임... 돈주고 보기에는....      0\n",
       "4  6723715  3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??      0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_table('ratings_test.txt', encoding='utf-8')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        50000 non-null  int64 \n",
      " 1   document  49997 non-null  object\n",
      " 2   label     50000 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "null 값 제거  \n",
    "중복 문장 제거  \n",
    "특수 문자 및 영어 제거  \n",
    "형태소 분리 후 Tokenizer 단어 집합 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "df_train['document'] = df_train['document'].str.replace('[^ㄱ-ㅎ가-힣 ]', ' ')\n",
    "df_train['document'] = df_train['document'].str.replace('[ ]{2,}', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_idx = df_train.loc[df_train['document'] == ' '].index\n",
    "df_train = df_train.drop(df_train_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop_duplicates(['document'], keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 48529 entries, 0 to 49999\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        48529 non-null  int64 \n",
      " 1   document  48529 non-null  object\n",
      " 2   label     48529 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 1.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Komoran\n",
    "tokenizer = Tokenizer()\n",
    "okt = Okt()\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[665],\n",
       " [87],\n",
       " [68],\n",
       " [159],\n",
       " [1],\n",
       " [23],\n",
       " [8],\n",
       " [11],\n",
       " [4677],\n",
       " [1543],\n",
       " [20],\n",
       " [929],\n",
       " [745],\n",
       " [9],\n",
       " [48],\n",
       " [848],\n",
       " [2567],\n",
       " [10406],\n",
       " [2678],\n",
       " [89],\n",
       " [361],\n",
       " [104],\n",
       " [110],\n",
       " [420],\n",
       " [142],\n",
       " [222],\n",
       " [16],\n",
       " [7846],\n",
       " [118],\n",
       " [1957],\n",
       " [105],\n",
       " [16207],\n",
       " [39],\n",
       " [19],\n",
       " [601],\n",
       " [157],\n",
       " [71717],\n",
       " [10],\n",
       " [5408],\n",
       " [6393],\n",
       " [203],\n",
       " [1],\n",
       " [3757],\n",
       " [145],\n",
       " [28],\n",
       " [3],\n",
       " [203],\n",
       " [2],\n",
       " [729],\n",
       " [73],\n",
       " [1114],\n",
       " [377],\n",
       " [281],\n",
       " [22],\n",
       " [71718],\n",
       " [13],\n",
       " [16208],\n",
       " [66],\n",
       " [1958],\n",
       " [3288],\n",
       " [2420],\n",
       " [4409],\n",
       " [10],\n",
       " [16209],\n",
       " [168],\n",
       " [440],\n",
       " [1248],\n",
       " [71719],\n",
       " [2421],\n",
       " [1028],\n",
       " [56],\n",
       " [265],\n",
       " [3289],\n",
       " [16210],\n",
       " [2837],\n",
       " [51],\n",
       " [7],\n",
       " [433],\n",
       " [2679],\n",
       " [4],\n",
       " [1249],\n",
       " [71720],\n",
       " [52],\n",
       " [2568],\n",
       " [16211],\n",
       " [84],\n",
       " [2],\n",
       " [5],\n",
       " [81],\n",
       " [383],\n",
       " [52],\n",
       " [2752],\n",
       " [41],\n",
       " [3153],\n",
       " [3],\n",
       " [1783],\n",
       " [429],\n",
       " [4],\n",
       " [12588],\n",
       " [915],\n",
       " [3154],\n",
       " [3],\n",
       " [7032],\n",
       " [202],\n",
       " [42],\n",
       " [1],\n",
       " [12589],\n",
       " [3970],\n",
       " [2288],\n",
       " [317],\n",
       " [16],\n",
       " [10407],\n",
       " [71721],\n",
       " [397],\n",
       " [65],\n",
       " [183],\n",
       " [1105],\n",
       " [3],\n",
       " [1410],\n",
       " [380],\n",
       " [649],\n",
       " [5],\n",
       " [71722],\n",
       " [534],\n",
       " [589],\n",
       " [1139],\n",
       " [11],\n",
       " [1189],\n",
       " [29],\n",
       " [1704],\n",
       " [280],\n",
       " [11],\n",
       " [4678],\n",
       " [6],\n",
       " [10408],\n",
       " [16212],\n",
       " [10409],\n",
       " [3],\n",
       " [1704],\n",
       " [280],\n",
       " [6],\n",
       " [32],\n",
       " [16213],\n",
       " [13],\n",
       " [5409],\n",
       " [332],\n",
       " [7847],\n",
       " [84],\n",
       " [6394],\n",
       " [2],\n",
       " [41],\n",
       " [237],\n",
       " [4],\n",
       " [1224],\n",
       " [167],\n",
       " [4410],\n",
       " [2],\n",
       " [3],\n",
       " [8893],\n",
       " [341],\n",
       " [11],\n",
       " [10410],\n",
       " [1175],\n",
       " [40],\n",
       " [107],\n",
       " [24],\n",
       " [1570],\n",
       " [1283],\n",
       " [414],\n",
       " [2],\n",
       " [3],\n",
       " [1544],\n",
       " [878],\n",
       " [6395],\n",
       " [2],\n",
       " [4],\n",
       " [150],\n",
       " [792],\n",
       " [6],\n",
       " [178],\n",
       " [36],\n",
       " [2],\n",
       " [71723],\n",
       " [71724],\n",
       " [53],\n",
       " [11],\n",
       " [27],\n",
       " [5],\n",
       " [2],\n",
       " [5864],\n",
       " [1],\n",
       " [337],\n",
       " [116],\n",
       " [16214],\n",
       " [8],\n",
       " [1],\n",
       " [27],\n",
       " [2],\n",
       " [10],\n",
       " [6396],\n",
       " [178],\n",
       " [84],\n",
       " [3290],\n",
       " [4],\n",
       " [71725],\n",
       " [12590],\n",
       " [2017],\n",
       " [1140],\n",
       " [3441],\n",
       " [9],\n",
       " [23],\n",
       " [11],\n",
       " [68],\n",
       " [191],\n",
       " [28],\n",
       " [626],\n",
       " [1082],\n",
       " [37],\n",
       " [1876],\n",
       " [17],\n",
       " [95],\n",
       " [24],\n",
       " [10],\n",
       " [37],\n",
       " [16215],\n",
       " [737],\n",
       " [7],\n",
       " [37],\n",
       " [1911],\n",
       " [16128],\n",
       " [118],\n",
       " [61],\n",
       " [7],\n",
       " [4174],\n",
       " [1642],\n",
       " [7848],\n",
       " [71726],\n",
       " [22],\n",
       " [558],\n",
       " [673],\n",
       " [12591],\n",
       " [3966],\n",
       " [139],\n",
       " [3],\n",
       " [5410],\n",
       " [8894],\n",
       " [220],\n",
       " [34],\n",
       " [441],\n",
       " [204],\n",
       " [253],\n",
       " [220],\n",
       " [10],\n",
       " [8895],\n",
       " [71727],\n",
       " [71728],\n",
       " [1],\n",
       " [83],\n",
       " [220],\n",
       " [3],\n",
       " [656],\n",
       " [148],\n",
       " [300],\n",
       " [3],\n",
       " [1127],\n",
       " [1],\n",
       " [2061],\n",
       " [76],\n",
       " [1094],\n",
       " [71729],\n",
       " [19],\n",
       " [5865],\n",
       " [16216],\n",
       " [71730],\n",
       " [11],\n",
       " [100],\n",
       " [5034],\n",
       " [573],\n",
       " [628],\n",
       " [3971],\n",
       " [462],\n",
       " [18],\n",
       " [41],\n",
       " [302],\n",
       " [666],\n",
       " [71731],\n",
       " [182],\n",
       " [809],\n",
       " [4411],\n",
       " [83],\n",
       " [23],\n",
       " [71732],\n",
       " [66],\n",
       " [83],\n",
       " [485],\n",
       " [1141],\n",
       " [214],\n",
       " [567],\n",
       " [121],\n",
       " [63],\n",
       " [2],\n",
       " [243],\n",
       " [5866],\n",
       " [374],\n",
       " [3],\n",
       " [12592],\n",
       " [1784],\n",
       " [3],\n",
       " [16217],\n",
       " [18],\n",
       " [31],\n",
       " [4679],\n",
       " [71733],\n",
       " [394],\n",
       " [18],\n",
       " [41],\n",
       " [247],\n",
       " [10411],\n",
       " [1],\n",
       " [377],\n",
       " [4412],\n",
       " [1571],\n",
       " [16218],\n",
       " [3],\n",
       " [5035],\n",
       " [972],\n",
       " [6],\n",
       " [2753],\n",
       " [49],\n",
       " [71734],\n",
       " [71735],\n",
       " [879],\n",
       " [916],\n",
       " [47],\n",
       " [12593],\n",
       " [188],\n",
       " [16219],\n",
       " [71736],\n",
       " [12594],\n",
       " [12595],\n",
       " [7033],\n",
       " [130],\n",
       " [1],\n",
       " [83],\n",
       " [1912],\n",
       " [71737],\n",
       " [2],\n",
       " [362],\n",
       " [33],\n",
       " [1959],\n",
       " [188],\n",
       " [1],\n",
       " [318],\n",
       " [71738],\n",
       " [803],\n",
       " [5],\n",
       " [71739],\n",
       " [880],\n",
       " [57],\n",
       " [1411],\n",
       " [29],\n",
       " [282],\n",
       " [313],\n",
       " [71740],\n",
       " [6397],\n",
       " [13],\n",
       " [282],\n",
       " [138],\n",
       " [282],\n",
       " [864],\n",
       " [916],\n",
       " [38],\n",
       " [71741],\n",
       " [5036],\n",
       " [2938],\n",
       " [69],\n",
       " [295],\n",
       " [530],\n",
       " [2062],\n",
       " [870],\n",
       " [2170],\n",
       " [95],\n",
       " [32],\n",
       " [2289],\n",
       " [540],\n",
       " [657],\n",
       " [848],\n",
       " [1065],\n",
       " [8896],\n",
       " [124],\n",
       " [3155],\n",
       " [1128],\n",
       " [109],\n",
       " [308],\n",
       " [64],\n",
       " [1],\n",
       " [16220],\n",
       " [81],\n",
       " [68],\n",
       " [1225],\n",
       " [249],\n",
       " [349],\n",
       " [1054],\n",
       " [2],\n",
       " [67],\n",
       " [46],\n",
       " [16221],\n",
       " [100],\n",
       " [60],\n",
       " [63],\n",
       " [2],\n",
       " [59],\n",
       " [162],\n",
       " [1],\n",
       " [204],\n",
       " [1785],\n",
       " [2],\n",
       " [12],\n",
       " [12],\n",
       " [286],\n",
       " [613],\n",
       " [128],\n",
       " [496],\n",
       " [23],\n",
       " [290],\n",
       " [1226],\n",
       " [12596],\n",
       " [2],\n",
       " [54],\n",
       " [30],\n",
       " [53],\n",
       " [262],\n",
       " [42],\n",
       " [146],\n",
       " [165],\n",
       " [5037],\n",
       " [22],\n",
       " [33],\n",
       " [71742],\n",
       " [168],\n",
       " [129],\n",
       " [23],\n",
       " [3758],\n",
       " [8],\n",
       " [12],\n",
       " [888],\n",
       " [681],\n",
       " [15],\n",
       " [17],\n",
       " [232],\n",
       " [554],\n",
       " [66],\n",
       " [2],\n",
       " [474],\n",
       " [1673],\n",
       " [96],\n",
       " [4],\n",
       " [364],\n",
       " [9],\n",
       " [10412],\n",
       " [1545],\n",
       " [4],\n",
       " [16222],\n",
       " [11],\n",
       " [37],\n",
       " [10413],\n",
       " [156],\n",
       " [61],\n",
       " [5],\n",
       " [1503],\n",
       " [64],\n",
       " [1],\n",
       " [1828],\n",
       " [6398],\n",
       " [9],\n",
       " [197],\n",
       " [3],\n",
       " [16223],\n",
       " [139],\n",
       " [193],\n",
       " [41],\n",
       " [93],\n",
       " [6399],\n",
       " [2359],\n",
       " [49],\n",
       " [23],\n",
       " [10414],\n",
       " [105],\n",
       " [4680],\n",
       " [339],\n",
       " [8],\n",
       " [3043],\n",
       " [546],\n",
       " [36],\n",
       " [1954],\n",
       " [84],\n",
       " [64],\n",
       " [663],\n",
       " [5],\n",
       " [52],\n",
       " [881],\n",
       " [17],\n",
       " [23],\n",
       " [1643],\n",
       " [118],\n",
       " [7034],\n",
       " [105],\n",
       " [7849],\n",
       " [9],\n",
       " [7849],\n",
       " [50],\n",
       " [133],\n",
       " [9],\n",
       " [133],\n",
       " [400],\n",
       " [205],\n",
       " [7850],\n",
       " [135],\n",
       " [200],\n",
       " [1740],\n",
       " [283],\n",
       " [2838],\n",
       " [1367],\n",
       " [71743],\n",
       " [7035],\n",
       " [3585],\n",
       " [113],\n",
       " [71744],\n",
       " [473],\n",
       " [68],\n",
       " [159],\n",
       " [180],\n",
       " [51],\n",
       " [4175],\n",
       " [1],\n",
       " [38],\n",
       " [2],\n",
       " [159],\n",
       " [281],\n",
       " [9],\n",
       " [247],\n",
       " [1],\n",
       " [10415],\n",
       " [7036],\n",
       " [71745],\n",
       " [3],\n",
       " [7037],\n",
       " [3],\n",
       " [20],\n",
       " [44],\n",
       " [18],\n",
       " [41],\n",
       " [12597],\n",
       " [5867],\n",
       " [3],\n",
       " [12598],\n",
       " [162],\n",
       " [1],\n",
       " [6],\n",
       " [2422],\n",
       " [497],\n",
       " [11],\n",
       " [125],\n",
       " [71746],\n",
       " [9],\n",
       " [1542],\n",
       " [664],\n",
       " [94],\n",
       " [404],\n",
       " [1],\n",
       " [650],\n",
       " [71747],\n",
       " [667],\n",
       " [29],\n",
       " [320],\n",
       " [9],\n",
       " [973],\n",
       " [10],\n",
       " [8],\n",
       " [128],\n",
       " [12599],\n",
       " [9],\n",
       " [917],\n",
       " [64],\n",
       " [6],\n",
       " [1829],\n",
       " [578],\n",
       " [974],\n",
       " [716],\n",
       " [10],\n",
       " [29],\n",
       " [16224],\n",
       " [198],\n",
       " [14],\n",
       " [182],\n",
       " [133],\n",
       " [9],\n",
       " [16225],\n",
       " [10416],\n",
       " [4],\n",
       " [202],\n",
       " [25],\n",
       " [6],\n",
       " [3972],\n",
       " [1913],\n",
       " [2171],\n",
       " [452],\n",
       " [287],\n",
       " [130],\n",
       " [71748],\n",
       " [1467],\n",
       " [819],\n",
       " [12600],\n",
       " [38],\n",
       " [685],\n",
       " [203],\n",
       " [1],\n",
       " [71749],\n",
       " [36],\n",
       " [825],\n",
       " [810],\n",
       " [1737],\n",
       " [2839],\n",
       " [12571],\n",
       " [16226],\n",
       " [983],\n",
       " [48],\n",
       " [3044],\n",
       " [71750],\n",
       " [71751],\n",
       " [233],\n",
       " [110],\n",
       " [16],\n",
       " [1238],\n",
       " [27],\n",
       " [5],\n",
       " [1],\n",
       " [25],\n",
       " [502],\n",
       " [8897],\n",
       " [71752],\n",
       " [1008],\n",
       " [87],\n",
       " [16227],\n",
       " [34],\n",
       " [71753],\n",
       " [3],\n",
       " [5038],\n",
       " [96],\n",
       " [11],\n",
       " [564],\n",
       " [8898],\n",
       " [4413],\n",
       " [27],\n",
       " [106],\n",
       " [406],\n",
       " [98],\n",
       " [43],\n",
       " [127],\n",
       " [57],\n",
       " [81],\n",
       " [71754],\n",
       " [22],\n",
       " [4681],\n",
       " [4414],\n",
       " [19],\n",
       " [12601],\n",
       " [168],\n",
       " [52],\n",
       " [284],\n",
       " [22],\n",
       " [360],\n",
       " [234],\n",
       " [179],\n",
       " [71755],\n",
       " [1142],\n",
       " [481],\n",
       " [46],\n",
       " [3291],\n",
       " [620],\n",
       " [5039],\n",
       " [29],\n",
       " [47],\n",
       " [5],\n",
       " [2479],\n",
       " [5],\n",
       " [252],\n",
       " [221],\n",
       " [286],\n",
       " [123],\n",
       " [2358],\n",
       " [23],\n",
       " [1284],\n",
       " [2236],\n",
       " [469],\n",
       " [466],\n",
       " [62],\n",
       " [3973],\n",
       " [53],\n",
       " [8],\n",
       " [8899],\n",
       " [48],\n",
       " [918],\n",
       " [1],\n",
       " [704],\n",
       " [337],\n",
       " [109],\n",
       " [1129],\n",
       " [5411],\n",
       " [449],\n",
       " [71756],\n",
       " [50],\n",
       " [5040],\n",
       " [7851],\n",
       " [2680],\n",
       " [1],\n",
       " [602],\n",
       " [415],\n",
       " [541],\n",
       " [286],\n",
       " [56],\n",
       " [7],\n",
       " [1441],\n",
       " [319],\n",
       " [7],\n",
       " [8900],\n",
       " [24],\n",
       " [7],\n",
       " [272],\n",
       " [259],\n",
       " [7],\n",
       " [221],\n",
       " [2115],\n",
       " [1468],\n",
       " [12602],\n",
       " [1914],\n",
       " [43],\n",
       " [8901],\n",
       " [71757],\n",
       " [975],\n",
       " [68],\n",
       " [782],\n",
       " [9],\n",
       " [645],\n",
       " [6400],\n",
       " [694],\n",
       " [1],\n",
       " [71758],\n",
       " [772],\n",
       " [5],\n",
       " [950],\n",
       " [686],\n",
       " [2237],\n",
       " [71759],\n",
       " [71760],\n",
       " [2],\n",
       " [923],\n",
       " [11],\n",
       " [170],\n",
       " [705],\n",
       " [16228],\n",
       " [3156],\n",
       " [16229],\n",
       " [71761],\n",
       " [1],\n",
       " [85],\n",
       " [466],\n",
       " [295],\n",
       " [37],\n",
       " [1271],\n",
       " [12603],\n",
       " [893],\n",
       " [894],\n",
       " [22],\n",
       " [71],\n",
       " [137],\n",
       " [113],\n",
       " [352],\n",
       " [324],\n",
       " [12604],\n",
       " [4682],\n",
       " [1],\n",
       " [38],\n",
       " [17],\n",
       " [10417],\n",
       " [17],\n",
       " [19],\n",
       " [64],\n",
       " [12605],\n",
       " [673],\n",
       " [6],\n",
       " [113],\n",
       " [738],\n",
       " [405],\n",
       " [286],\n",
       " [291],\n",
       " [32],\n",
       " [273],\n",
       " [288],\n",
       " [882],\n",
       " [6],\n",
       " [10418],\n",
       " [16],\n",
       " [964],\n",
       " [75],\n",
       " [65],\n",
       " [164],\n",
       " [5868],\n",
       " [7852],\n",
       " [7038],\n",
       " [12],\n",
       " [1227],\n",
       " [124],\n",
       " [7853],\n",
       " [169],\n",
       " [2237],\n",
       " [11],\n",
       " [3045],\n",
       " [218],\n",
       " [10419],\n",
       " [156],\n",
       " [6401],\n",
       " [4],\n",
       " [134],\n",
       " [2238],\n",
       " [10420],\n",
       " [7],\n",
       " [811],\n",
       " [724],\n",
       " [16230],\n",
       " [38],\n",
       " [8902],\n",
       " [51],\n",
       " [127],\n",
       " [238],\n",
       " [92],\n",
       " [3],\n",
       " [3292],\n",
       " [1960],\n",
       " [181],\n",
       " [10421],\n",
       " [9],\n",
       " [365],\n",
       " [202],\n",
       " [98],\n",
       " [812],\n",
       " [21],\n",
       " [1029],\n",
       " [4415],\n",
       " [22],\n",
       " [16231],\n",
       " [700],\n",
       " [3157],\n",
       " [143],\n",
       " [2116],\n",
       " [813],\n",
       " [49],\n",
       " [29],\n",
       " [202],\n",
       " [71762],\n",
       " [6],\n",
       " [71763],\n",
       " [1083],\n",
       " [21],\n",
       " [2569],\n",
       " [185],\n",
       " [10],\n",
       " [71764],\n",
       " [1018],\n",
       " [71765],\n",
       " [706],\n",
       " [282],\n",
       " [8],\n",
       " [16232],\n",
       " [75],\n",
       " [107],\n",
       " [341],\n",
       " [2480],\n",
       " [40],\n",
       " [2480],\n",
       " [486],\n",
       " [2480],\n",
       " [102],\n",
       " [1469],\n",
       " [739],\n",
       " [46],\n",
       " [1349],\n",
       " [10422],\n",
       " [237],\n",
       " [3293],\n",
       " [7],\n",
       " [68],\n",
       " [81],\n",
       " [68],\n",
       " [224],\n",
       " [478],\n",
       " [12606],\n",
       " [12569],\n",
       " [74],\n",
       " [58],\n",
       " [871],\n",
       " [47],\n",
       " [113],\n",
       " [61],\n",
       " [66],\n",
       " [2],\n",
       " [8903],\n",
       " [8903],\n",
       " [8903],\n",
       " [8903],\n",
       " [36],\n",
       " [56],\n",
       " [89],\n",
       " [221],\n",
       " [16233],\n",
       " [773],\n",
       " [172],\n",
       " [645],\n",
       " [71766],\n",
       " [250],\n",
       " [395],\n",
       " [106],\n",
       " [16234],\n",
       " [939],\n",
       " [2063],\n",
       " [637],\n",
       " [74],\n",
       " [16],\n",
       " [171],\n",
       " [4416],\n",
       " [339],\n",
       " [5],\n",
       " [4683],\n",
       " [71767],\n",
       " [71768],\n",
       " [51],\n",
       " [1],\n",
       " [3759],\n",
       " [230],\n",
       " [81],\n",
       " [195],\n",
       " [29],\n",
       " [646],\n",
       " [3],\n",
       " [2],\n",
       " [2064],\n",
       " [171],\n",
       " [166],\n",
       " [1572],\n",
       " [86],\n",
       " [4176],\n",
       " [8],\n",
       " [26],\n",
       " [1350],\n",
       " [6402],\n",
       " [5],\n",
       " [4684],\n",
       " [12607],\n",
       " [175],\n",
       " [4],\n",
       " [71769],\n",
       " [71770],\n",
       " [263],\n",
       " [1],\n",
       " [71771],\n",
       " [93],\n",
       " [109],\n",
       " [76],\n",
       " [262],\n",
       " [38],\n",
       " [10423],\n",
       " [1305],\n",
       " [85],\n",
       " [19],\n",
       " [3586],\n",
       " [16235],\n",
       " [4417],\n",
       " [5412],\n",
       " [3],\n",
       " [3974],\n",
       " [774],\n",
       " [849],\n",
       " [71772],\n",
       " [72],\n",
       " [16236],\n",
       " [1502],\n",
       " [137],\n",
       " [804],\n",
       " [3156],\n",
       " [275],\n",
       " [235],\n",
       " [4],\n",
       " [755],\n",
       " [1],\n",
       " [67],\n",
       " [71773],\n",
       " [2117],\n",
       " [172],\n",
       " [802],\n",
       " [1],\n",
       " ...]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.fit_on_texts(temp3) # sentences는 항상 2차원이어야 한다\n",
    "tokenizer.texts_to_sequences(temp3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모범답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    0\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_table('ratings_test.txt', encoding='utf-8')\n",
    "df_train.head()\n",
    "\n",
    "df_train = df_train.drop_duplicates(subset=['document'])\n",
    "df_train = df_train.dropna(how='any')\n",
    "\n",
    "df_train['document'] = df_train['document'].str.replace('[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '')\n",
    "\n",
    "df_train['document'].replace('', np.nan, inplace=True)\n",
    "df_train = df_train.dropna(how='any')\n",
    "\n",
    "# 확인\n",
    "df_train['document'].nunique() # 중복 제외한 개수\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### stemming, norm -> stopword가 일반적!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['굳다', 'ㅋ'],\n",
       " ['뭐',\n",
       "  '야',\n",
       "  '이',\n",
       "  '평점',\n",
       "  '들',\n",
       "  '은',\n",
       "  '나쁘다',\n",
       "  '않다',\n",
       "  '점',\n",
       "  '짜다',\n",
       "  '리',\n",
       "  '는',\n",
       "  '더',\n",
       "  '더욱',\n",
       "  '아니다'],\n",
       " ['지루하다', '않다', '완전', '막장', '임', '돈', '주다', '보기', '에는']]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "\n",
    "X_train = []\n",
    "\n",
    "for sent in df_train['document']:\n",
    "    temp_X = []\n",
    "    temp_X = okt.morphs(sent, stem=True, norm=True)\n",
    "    temp_X = [w for w in temp_X if not w in stopwords]\n",
    "    \n",
    "    X_train.append(temp_X)\n",
    "\n",
    "X_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[637, 108],\n",
       " [76, 184, 1, 36, 11, 10, 406, 32, 33, 295, 778, 12, 56, 872, 29],\n",
       " [82, 32, 107, 390, 126, 127, 74, 164, 261]]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "tokenizer.texts_to_sequences(X_train)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF 실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, document):\n",
    "    count = 0\n",
    "    for i in document.split():\n",
    "        if i == term:\n",
    "            count = count + 1 \n",
    "    return count    \n",
    "\n",
    "def document_freq(term, documents):\n",
    "    count = 0\n",
    "    for i in documents:\n",
    "        if term in i:\n",
    "            count = count + len([i])\n",
    "    return count\n",
    "    \n",
    "\n",
    "def idf(term, documents):\n",
    "    from math import log\n",
    "    count = document_freq(term, documents)\n",
    "#     n = 0\n",
    "#     for i in documents:\n",
    "#         for j in i.split():\n",
    "#             n = n + len([j])\n",
    "    return log(len(documents) / (1 + count))\n",
    "\n",
    "\n",
    "\n",
    "def tf_idf(term, documents, idx):\n",
    "    idf_tmp = idf(term, documents)\n",
    "    tf_tmp = term_freq(term, documents[idx])\n",
    "    return tf_tmp*idf_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = '동해 물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 동해 가고 싶다'\n",
    "\n",
    "docs = [\n",
    "  '동해 물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 동해 가고 싶다',\n",
    "  '남산 위에 저 소나무, 철갑을 두른 듯 바람 서리 불변함은 우리 기상일세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 소나무 이쁘다',\n",
    "  '가을 하늘 공활한데 높고 구름 없이 밝은 달은 우리 가슴 일편단심일세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 가을 하늘 보고 싶다.',\n",
    "  '이 기상과 이 마음으로 충성을 다하여 괴로우나 즐거우나 나라 사랑하세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 나라를 사랑하자'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_freq('동해', doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_freq('동해', docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6931471805599453"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf('동해', docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3862943611198906"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf('하늘', docs, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 모범답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_freq(term, document):\n",
    "    return document.count(term)\n",
    "\n",
    "\n",
    "def document_freq(term, documents):\n",
    "    count = 0\n",
    "    for i in documents:\n",
    "        # True = 1, False = 0 으로 term이 나옴\n",
    "        count += term in i\n",
    "    return count\n",
    "    \n",
    "\n",
    "def idf(term, documents):\n",
    "    from math import log\n",
    "    count = document_freq(term, documents)\n",
    "\n",
    "    n = len(set(' '.join(documents).split()))\n",
    "    \n",
    "    return log(len(documents) / (1 + count))\n",
    "\n",
    "\n",
    "def tf_idf(term, documents, idx):\n",
    "    idf_tmp = idf(term, documents)\n",
    "    tf_tmp = term_freq(term, documents[idx])\n",
    "    return tf_tmp*idf_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF 전처리 및 라이브러리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "  '동해 물과 백두산이 마르고 닳도록 하느님이 보우하사 우리나라 만세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 동해 가고 싶다',\n",
    "  '남산 위에 저 소나무, 철갑을 두른 듯 바람 서리 불변함은 우리 기상일세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 소나무 이쁘다',\n",
    "  '가을 하늘 공활한데 높고 구름 없이 밝은 달은 우리 가슴 일편단심일세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 가을 하늘 보고 싶다.',\n",
    "  '이 기상과 이 마음으로 충성을 다하여 괴로우나 즐거우나 나라 사랑하세. 무궁화 삼천리 화려 강산 대한 사람, 대한으로 길이 보전하세. 나라를 사랑하자'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['보우', '만세', '물', '이', '대한', '소나무', '보전', '사랑', '불변', '동해', '화려', '무궁화', '남산', '사람', '일편단심', '하느님', '하사', '마르고', '함', '가슴', '우리나라', '강산', '데', '달', '활', '길이', '하늘', '우리', '충성', '마음', '저', '가을', '나라', '철갑', '바람', '서리', '보고', '듯', '위', '구름', '기상', '백두산', '삼천리']\n"
     ]
    }
   ],
   "source": [
    "vocab = list(set(w for doc in docs for w in okt.nouns(doc)))\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 가나다 순으로 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['가슴', '가을', '강산']"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.sort()\n",
    "vocab[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTM 실습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하나의 sentence 마다 빈도 벡터를 구한다\n",
    "- 모두 모은다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가슴</th>\n",
       "      <th>가을</th>\n",
       "      <th>강산</th>\n",
       "      <th>구름</th>\n",
       "      <th>기상</th>\n",
       "      <th>길이</th>\n",
       "      <th>나라</th>\n",
       "      <th>남산</th>\n",
       "      <th>달</th>\n",
       "      <th>대한</th>\n",
       "      <th>...</th>\n",
       "      <th>일편단심</th>\n",
       "      <th>저</th>\n",
       "      <th>철갑</th>\n",
       "      <th>충성</th>\n",
       "      <th>하느님</th>\n",
       "      <th>하늘</th>\n",
       "      <th>하사</th>\n",
       "      <th>함</th>\n",
       "      <th>화려</th>\n",
       "      <th>활</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   가슴  가을  강산  구름  기상  길이  나라  남산  달  대한  ...  일편단심  저  철갑  충성  하느님  하늘  하사  \\\n",
       "0   0   0   1   0   0   1   1   0  0   2  ...     0  0   0   0    1   0   1   \n",
       "1   0   0   1   0   1   1   0   1  0   2  ...     0  1   1   0    0   0   0   \n",
       "2   1   2   1   1   0   1   0   0  1   2  ...     1  0   0   0    0   2   0   \n",
       "3   0   0   1   0   1   1   2   0  0   2  ...     0  0   0   1    0   0   0   \n",
       "\n",
       "   함  화려  활  \n",
       "0  0   1  0  \n",
       "1  1   1  0  \n",
       "2  0   1  1  \n",
       "3  0   1  0  \n",
       "\n",
       "[4 rows x 43 columns]"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst1 = []\n",
    "lst2 = []\n",
    "lst3 = []\n",
    "lst4 = []\n",
    "for i in range(len(vocab)):\n",
    "    lst1.append(term_freq(vocab[i], docs[0]))\n",
    "for i in range(len(vocab)):\n",
    "    lst2.append(term_freq(vocab[i], docs[1]))\n",
    "for i in range(len(vocab)):\n",
    "    lst3.append(term_freq(vocab[i], docs[2]))\n",
    "for i in range(len(vocab)):\n",
    "    lst4.append(term_freq(vocab[i], docs[3]))\n",
    "# print(lst1, lst2, lst3, lst4)\n",
    "# print(len(lst1), len(lst2), len(lst3), len(lst4))\n",
    "    \n",
    "tmp = pd.DataFrame([lst1, lst2, lst3, lst4], columns=vocab)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTM 모범답안"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# list comprehension\n",
    "pd.DataFrame([list(map(lambda x: term_freq(x, doc), vocab)) for doc in docs], columns=vocab)\n",
    "\n",
    "# for문\n",
    "res = []\n",
    "for doc in docs:\n",
    "    tf_res = []\n",
    "    for i in vocab:\n",
    "        tf_res.append(term_freq(i, doc))\n",
    "    res.append(tf_res)\n",
    "    \n",
    "fin = pd.DataFrame(res, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IDF 결과를 DataFrame으로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>가슴</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>가을</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>강산</th>\n",
       "      <td>-0.223144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>구름</th>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>기상</th>\n",
       "      <td>0.287682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         IDF\n",
       "가슴  0.693147\n",
       "가을  0.693147\n",
       "강산 -0.223144\n",
       "구름  0.693147\n",
       "기상  0.287682"
      ]
     },
     "execution_count": 356,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for term in vocab:\n",
    "    res.append(idf(term, docs))\n",
    "\n",
    "pd.DataFrame(res, index=vocab, columns=['IDF']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF 결과를 DataFrame으로 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>가슴</th>\n",
       "      <th>가을</th>\n",
       "      <th>강산</th>\n",
       "      <th>구름</th>\n",
       "      <th>기상</th>\n",
       "      <th>길이</th>\n",
       "      <th>나라</th>\n",
       "      <th>남산</th>\n",
       "      <th>달</th>\n",
       "      <th>대한</th>\n",
       "      <th>...</th>\n",
       "      <th>일편단심</th>\n",
       "      <th>저</th>\n",
       "      <th>철갑</th>\n",
       "      <th>충성</th>\n",
       "      <th>하느님</th>\n",
       "      <th>하늘</th>\n",
       "      <th>하사</th>\n",
       "      <th>함</th>\n",
       "      <th>화려</th>\n",
       "      <th>활</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.446287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.446287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>-0.446287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.386294</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.693147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.287682</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.575364</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.446287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.693147</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.223144</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         가슴        가을        강산        구름        기상        길이        나라  \\\n",
       "0  0.000000  0.000000 -0.223144  0.000000  0.000000 -0.223144  0.287682   \n",
       "1  0.000000  0.000000 -0.223144  0.000000  0.287682 -0.223144  0.000000   \n",
       "2  0.693147  1.386294 -0.223144  0.693147  0.000000 -0.223144  0.000000   \n",
       "3  0.000000  0.000000 -0.223144  0.000000  0.287682 -0.223144  0.575364   \n",
       "\n",
       "         남산         달        대한  ...      일편단심         저        철갑        충성  \\\n",
       "0  0.000000  0.000000 -0.446287  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.693147  0.000000 -0.446287  ...  0.000000  0.693147  0.693147  0.000000   \n",
       "2  0.000000  0.693147 -0.446287  ...  0.693147  0.000000  0.000000  0.000000   \n",
       "3  0.000000  0.000000 -0.446287  ...  0.000000  0.000000  0.000000  0.693147   \n",
       "\n",
       "        하느님        하늘        하사         함        화려         활  \n",
       "0  0.693147  0.000000  0.693147  0.000000 -0.223144  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.693147 -0.223144  0.000000  \n",
       "2  0.000000  1.386294  0.000000  0.000000 -0.223144  0.693147  \n",
       "3  0.000000  0.000000  0.000000  0.000000 -0.223144  0.000000  \n",
       "\n",
       "[4 rows x 43 columns]"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "for i in range(len(docs)):\n",
    "    tf_idf_res = []\n",
    "    for term in vocab:\n",
    "        tf_idf_res.append(tf_idf(term, docs, i))\n",
    "    res.append(tf_idf_res)\n",
    "\n",
    "pd.DataFrame(res, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow로 BoW 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'무궁화': 1, '삼천리': 2, '화려': 3, '강산': 4, '대한': 5, '사람': 6, '대한으로': 7, '길이': 8, '보전하세': 9, '동해': 10, '싶다': 11, '소나무': 12, '우리': 13, '가을': 14, '하늘': 15, '이': 16, '물과': 17, '백두산이': 18, '마르고': 19, '닳도록': 20, '하느님이': 21, '보우하사': 22, '우리나라': 23, '만세': 24, '가고': 25, '남산': 26, '위에': 27, '저': 28, '철갑을': 29, '두른': 30, '듯': 31, '바람': 32, '서리': 33, '불변함은': 34, '기상일세': 35, '이쁘다': 36, '공활한데': 37, '높고': 38, '구름': 39, '없이': 40, '밝은': 41, '달은': 42, '가슴': 43, '일편단심일세': 44, '보고': 45, '기상과': 46, '마음으로': 47, '충성을': 48, '다하여': 49, '괴로우나': 50, '즐거우나': 51, '나라': 52, '사랑하세': 53, '나라를': 54, '사랑하자': 55}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "t = Tokenizer()\n",
    "# 형태소 분리 안 한 상태로 ㄱㄱ\n",
    "t.fit_on_texts(docs)\n",
    "\n",
    "print(t.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DTM 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 1. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1.\n",
      "  1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 2. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 1. 0. 1. 2. 2. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 2. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
      "  1. 1. 1. 1. 1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(t.texts_to_matrix(docs, mode='count'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>0.587787</td>\n",
       "      <td>...</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "      <td>1.098612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0         1         2         3         4         5         6         7   \\\n",
       "0  0.0  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787   \n",
       "1  0.0  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787   \n",
       "2  0.0  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787   \n",
       "3  0.0  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787  0.587787   \n",
       "\n",
       "         8         9   ...        46        47        48        49        50  \\\n",
       "0  0.587787  0.587787  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.587787  0.587787  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.587787  0.587787  ...  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3  0.587787  0.587787  ...  1.098612  1.098612  1.098612  1.098612  1.098612   \n",
       "\n",
       "         51        52        53        54        55  \n",
       "0  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "2  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3  1.098612  1.098612  1.098612  1.098612  1.098612  \n",
       "\n",
       "[4 rows x 56 columns]"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(t.texts_to_matrix(docs, mode='tfidf'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "217.482px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 162.848,
   "position": {
    "height": "40px",
    "left": "629.429px",
    "right": "20px",
    "top": "-3.99999px",
    "width": "414px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
