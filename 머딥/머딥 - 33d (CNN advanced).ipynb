{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "746019ee",
   "metadata": {},
   "source": [
    "## Residual Unit\n",
    "\n",
    "bn - relu - weight(conv) - bn - relu - weight(conv) - addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a05fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59616e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        \n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        # 생성자에서 relu는 안 만들 것이다.\n",
    "        # 왜냐하면 학습 가능한 파라미터가 없기 때문에\n",
    "        # 사용(call)할 때만 구성해 주\"면 된다\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        # channel 개수가 서로 같은 경우\n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x # identity mapping\n",
    "        \n",
    "        # channel 개수가 서로 다른 경우\n",
    "        else:\n",
    "            # 입력 데이터의 ch을 filter_out 채널 개수로 변경\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1, 1), padding='same')\n",
    "            \n",
    "    def call(self, x, training=False):\n",
    "        # training 정말 중요!\n",
    "        # 왜? BN은 훈련때와 추론할 때가 다르기 때문에\n",
    "        h = self.bn1(x, training=training) # 추론과정에서는 False, 훈련과정에서는 True\n",
    "        h = tf.nn.relu(h) #nn = neural net\n",
    "        h = self.conv1(h)\n",
    "\n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "\n",
    "        return self.identity(x) + h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dc7e42",
   "metadata": {},
   "source": [
    "## Residual Layer\n",
    "residual unit을 이용해서 feature extraction 하는 레이어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df8f31a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Layer?? residual unit 여러개를 묶은 것\n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    # Residual Unit을 여러개 사용할 것이기 때문에\n",
    "    # Filters는 리스트 형태로 한 번에 저장해둔다\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequence = list()\n",
    "        \n",
    "        # [16] + [32, 32, 32] 이면?\n",
    "        # res unit은 3개, skip connection은 3단계\n",
    "        for f_in, f_out in zip([filter_in] + list(filters), filters):\n",
    "            # zip 안에서 list길이가 서로 다르면 짧은 쪽의 len만큼 loop가 돈다\n",
    "            \n",
    "            # for i, j in zip([1,2,3,4], [1,2,3]):\n",
    "            #     print(i, j)\n",
    "            self.sequence.append(ResidualUnit(f_in, f_out, kernel_size))  \n",
    "            \n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        for unit in self.sequence:\n",
    "            # ResidualUnit클래스 객체 만든 것 아래서 호출하기\n",
    "            x = unit(x, training=False) # unit의 call호출됨\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e92e00c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 32, 32, 32]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 더하면 어케 될까?\n",
    "filter_in = 16\n",
    "filters = [32, 32, 32]\n",
    "\n",
    "[filter_in] + list(filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2705df9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n",
      "2 2\n",
      "3 3\n"
     ]
    }
   ],
   "source": [
    "# 몇 번 돌까?\n",
    "for i, j in zip([1,2,3,4], [1,2,3]):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c92d817",
   "metadata": {},
   "source": [
    "ResnetLayer(filter_in, (after_res_unit_in, after_res_unit_in), kernel_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c53207",
   "metadata": {},
   "source": [
    "## ResNet 종합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55d0d90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        # feature extraction = Conv으로 초기 feature 뽑기\n",
    "        # output은 28 x 28 x 8 유지됨 ㅇㅇ (mnist 기준)\n",
    "        self.conv1 = tf.keras.layers.Conv2D(8, (3, 3), padding='same', activation='relu') \n",
    "        \n",
    "        # filters, resnet layer 두 개 잖아 그러니 filter도 2개! , kernel_size\n",
    "        self.res1 = ResnetLayer(8, (16, 16), (3, 3)) # output(28, 28, 16)\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2, 2)) # output(14, 14, 16)\n",
    "        \n",
    "        self.res2 = ResnetLayer(16, (32, 32), (3, 3)) # output(14, 14, 32)\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2)) # output(7, 7, 32)\n",
    "        \n",
    "        self.res3 = ResnetLayer(32, (64, 64), (3, 3)) # output(7, 7, 64)\n",
    "        \n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    \n",
    "    # training 변수 반드시...!\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "       \n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = self.res3(x, training=training)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bacc51cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train, X_test = X_train/255., X_test/255.\n",
    "\n",
    "X_train = X_train[..., tf.newaxis]\n",
    "X_test = X_test[..., tf.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62a8514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1024).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3460284a",
   "metadata": {},
   "source": [
    "## 자동미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "50d1d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(model, images, labels, loss_object, optimizer, train_loss, train_accuracy):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 훈련 과정이므로 BN True로 ㄱㄱ\n",
    "        prediction = model(images, training=True)\n",
    "        loss = loss_object(labels, prediction)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, prediction)\n",
    "\n",
    "# inference 단계    \n",
    "@tf.function\n",
    "def test_step(model, images, labels, loss_object, optimizer, test_loss, test_accuracy):\n",
    "    # BN 학습 ㄴㄴ\n",
    "    prediction = model(images, training=False)\n",
    "    t_loss = loss_object(labels, prediction)\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cfce6521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = ResNet()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.RMSprop(lr=1e-2)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f402bd1a",
   "metadata": {},
   "source": [
    "## 연습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fb635b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class ResidualUnit(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filter_out, kernel_size):\n",
    "        super(ResidualUnit, self).__init__()\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.conv2 = tf.keras.layers.Conv2D(filter_out, kernel_size, padding='same')\n",
    "        \n",
    "        if filter_in == filter_out:\n",
    "            self.identity = lambda x: x\n",
    "        else:\n",
    "            self.identity = tf.keras.layers.Conv2D(filter_out, (1, 1), padding='same')\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        h = self.bn1(x, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv1(h)\n",
    "        h = self.bn2(h, training=training)\n",
    "        h = tf.nn.relu(h)\n",
    "        h = self.conv2(h)\n",
    "        return self.identity(x) + h\n",
    "    \n",
    "class ResnetLayer(tf.keras.Model):\n",
    "    def __init__(self, filter_in, filters, kernel_size):\n",
    "        super(ResnetLayer, self).__init__()\n",
    "        self.sequences = list()\n",
    "        for f_in, f_out in zip([filter_in]+list(filters), filters):\n",
    "            self.sequences.append(ResidualUnit(f_in, f_out, kernel_size))\n",
    "    def call(self, x, training=False):\n",
    "        for unit in self.sequences:\n",
    "            x = unit(x, training=training)\n",
    "        return x\n",
    "\n",
    "class ResNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResNet, self).__init__()\n",
    "        # feature extraction 용\n",
    "        self.conv1 = tf.keras.layers.Conv2D(6, (3, 3), padding='same', activation='relu')\n",
    "        self.res1 = ResnetLayer(6, (12, 12, 12), (3, 3))\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D((2,2))\n",
    "        self.res2 = ResnetLayer(12, (24, 24, 24), (3, 3))\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D((2, 2))\n",
    "        self.res3 = ResnetLayer(24, (48, 48, 48), (3, 3))\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.dense1 = tf.keras.layers.Dense(100, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(10, activation='softmax')\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.res1(x, training=training)\n",
    "        x = self.pool1(x)\n",
    "        x = self.res2(x, training=training)\n",
    "        x = self.pool2(x)\n",
    "        x = self.res3(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "\n",
    "@tf.function\n",
    "def train_step(model, x, y, loss_obj, optm, train_loss, train_acc):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(x, training=True)\n",
    "        loss = loss_obj(y, prediction)\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optm.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_acc(y, prediction)\n",
    "\n",
    "@tf.function\n",
    "def test_step(model, x, y, loss_obj, optm, test_loss, test_acc):\n",
    "    prediction = model(x, training=False)\n",
    "    t_loss = loss_obj(y, prediction)\n",
    "    test_loss(t_loss)\n",
    "    test_acc(y, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "900666b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train[..., tf.newaxis]/255.\n",
    "X_test = X_test[..., tf.newaxis]/255.\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(112).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4d18fa80",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet()\n",
    "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "optm = tf.keras.optimizers.Nadam(learning_rate=0.01)\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_acc')\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "test_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='test_acc')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef9c083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer res_net_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for x, y in train_ds:\n",
    "        train_step(model, x, y, loss_obj, optm, train_loss, train_acc)\n",
    "    for test_x, test_y in test_ds:\n",
    "        test_step(model, test_x, test_y, loss_obj, optm, test_loss, test_acc)\n",
    "    print(f'epoch : {epoch + 1}, train loss : {train_loss:.3f}, train acc : {train_acc:.3f}, test loss : {test_loss:.3f}, test acc : {test_acc:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9510c8c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.857px",
    "left": "704.429px",
    "right": "20px",
    "top": "16px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
